{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost incremental\n",
    "## ðŸ§¬ Serving\n",
    "\n",
    "Author: https://github.com/deburky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "from catboost_incremental.serve_ray import CatBoostModelDeployment\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Deploy the CatBoost model with Ray Serve\n",
    "model_path = \"../models/cb_model.cbm\"\n",
    "app = CatBoostModelDeployment.bind(model_path=model_path)\n",
    "\n",
    "# Start Ray Serve (no need for uvicorn here, as Serve is already managing the server)\n",
    "serve.start(detached=True, http_options={\"host\": \"0.0.0.0\", \"port\": 8000})\n",
    "\n",
    "# Run the app to expose the endpoint\n",
    "serve.run(app, route_prefix=\"/predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "API_URL = \"http://127.0.0.1:8000/predict\"\n",
    "DATA_PATH = \"../data\"\n",
    "CHUNK_SIZE = 50_000\n",
    "MAX_CONCURRENT_REQUESTS = 20\n",
    "\n",
    "\n",
    "def load_rows_in_chunks(path: str, chunk_size: int):\n",
    "    \"\"\"Yields row dictionaries in chunks from Parquet.\"\"\"\n",
    "    dataset = ds.dataset(path, format=\"parquet\")\n",
    "    for batch in dataset.to_batches(batch_size=chunk_size):\n",
    "        df = batch.to_pandas()\n",
    "        df = df.drop(columns=[\"partition_id\", \"target\"], errors=\"ignore\")\n",
    "        yield df.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "async def run_chunk(rows, chunk_idx, client, sem):\n",
    "    \"\"\"Async function to send a chunk of rows to the API.\"\"\"\n",
    "    async with sem:\n",
    "        try:\n",
    "            resp = await client.post(API_URL, json=rows)\n",
    "            print(f\"Chunk {chunk_idx} => {resp.json()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Chunk {chunk_idx} failed: {e}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run all chunks concurrently.\"\"\"\n",
    "    sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        tasks.extend(\n",
    "            run_chunk(rows, chunk_idx, client, sem)\n",
    "            for chunk_idx, rows in enumerate(load_rows_in_chunks(DATA_PATH, CHUNK_SIZE))\n",
    "        )\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "# Properly shut down Ray Serve and Ray\n",
    "serve.shutdown()\n",
    "ray.shutdown()\n",
    "\n",
    "print(\"Ray and Ray Serve have been shut down.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
